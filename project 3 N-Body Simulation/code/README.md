# CSC4005 Project 3 Template

<br/>

# Physics

We have some physics variable declared in `headers/physics.h`:

```c++
int bound_x = 4000;
int bound_y = 4000;
int max_mass = 40000000;
double error = 1e-9f;
double dt = 0.0001f;
double gravity_const = 1.0f;
double radius2 = 4.0f;
```

`gravity_const` is the gravity constant when you compute $F=G m_{i} m_{j} / d^2$.

`dt` is the time span between two iterations, it can be used when you compute $\Delta v=F\Delta t$ and $\Delta x=v\Delta t$.

`error` is a small number used to avoid `DivisionByZero` error. It can be used like $F=G m_{i} m_{j} / (d^2 + error)$.

`radius2` is the squared radius of particles. It can be used when you determine whether two particles have collision.

`bound_x` is the upper bound of X axis. $x$ is between $[0,bound\_x]$.

`bound_y` is the upper bound of Y axis. $y$ is between $[0,bound\_y]$.

`max_mass` is the maximum mass of a particle. You can use it to generate particles.

You may need to modify them to better visualize your result.

<br/>
<br/>
<br/>

# Workflow & Logger & Reproduce

## Workflow

Simulation with a large amount of bodies is impossible on your own VM, so it is necessay to simulate on cluster and then transfer the data back to your VM for visualization. We have implemented 2 utilities to help you do that.

Also, for CUDA implementation, there is no CUDA on our VM, so you can use cluster to generate simulation results first, then you can transfer the results back to your VM. You can visualize the results on your VM.

## Logger

We provide a utility called `Logger` at `headers/checkpoint.h`, it can save `x,y` coordinates of multiple frames. Result will be stored in `./checkpoints/xxxxxxx/`. 

Here we give a sample use:

```c++
Logger l = Logger("cuda", 10000, 4000, 4000);
for (int i = 0; i < n_iterations; i++){
    // compute x, y
    l.save_frame(x, y);
}
```


## Copy Results Back

You can use `scp` command. 

Execute the following command on your VM.

```bash
scp -r $STUDENT_ID@10.26.200.21:/nfsmnt/$STUDENT_ID/xxxxx .
```

here `-r` represent recursively copying all files within a given directory.

You will find the target directory on cluster appears in your local working directory.


## Reproduce

We have implemented `video.cpp` to help you visualize simulation results generated by cluster.  

Use

```bash
g++ ./src/video.cpp -o video -I/usr/include -L/usr/local/lib -L/usr/lib -lglut -lGLU -lGL -lm -DGUI -O2 -std=c++11
```

to compile `video` GUI application on your VM.

Once you get a checkpoint directory like `./checkpoints/sequential_1000_20221107025155` (from cluster or somewhere else), you can use

```bash
./video ./checkpoints/sequential_1000_20221107025155
```

to reproduce result with GUI on your VM. 

**So, you can visualize the output of your CUDA program!**

<br/>
<br/>
<br/>
<br/>


# Compile & Run

## Compile

Sequential (command line application):

```bash
g++ ./src/sequential.cpp -o seq -O2 -std=c++11
```

Sequential (GUI application):

```bash
g++ ./src/sequential.cpp -o seqg -I/usr/include -L/usr/local/lib -L/usr/lib -lglut -lGLU -lGL -lm -DGUI -O2 -std=c++11
```

MPI (command line application):

```bash
mpic++ ./src/mpi.cpp -o mpi -std=c++11
```

MPI (GUI application):

```bash
mpic++ ./src/mpi.cpp -o mpig -I/usr/include -L/usr/local/lib -L/usr/lib -lglut -lGLU -lGL -lm -DGUI -std=c++11
```

Pthread (command line application):

```bash
g++ ./src/pthread.cpp -o pthread -lpthread -O2 -std=c++11
```

Pthread (GUI application):

```bash
g++ ./src/pthread.cpp -o pthreadg -I/usr/include -L/usr/local/lib -L/usr/lib -lglut -lGLU -lGL -lm -lpthread -DGUI -O2 -std=c++11
```

CUDA (command line application): notice that `nvcc` is not available on VM, please use cluster.

```bash
nvcc ./src/cuda.cu -o cuda -O2 --std=c++11
```

CUDA (GUI application): notice that `nvcc` is not available on VM, please use cluster.

```bash
nvcc ./src/cuda.cu -o cudag -I/usr/include -L/usr/local/lib -L/usr/lib -lglut -lGLU -lGL -lm -O2 -DGUI --std=c++11
```


OpenMP (command line application):

```bash
g++ ./src/openmp.cpp -o openmp -fopenmp -O2 -std=c++11
```

OpenMP (GUI application):

```bash
g++ ./src/openmp.cpp -o openmpg -fopenmp -I/usr/include -L/usr/local/lib -L/usr/lib -lglut -lGLU -lGL -lm -O2 -DGUI -std=c++11
```


## Run

Sequential (command line mode):

```bash
./seq $n_body $n_iterations
```

Sequential (GUI mode): please run this on VM (with GUI desktop).

```bash
./seqg $n_body $n_iterations
```

MPI (command line mode):

```bash
mpirun -np $n_processes ./mpi $n_body $n_iterations
```

MPI (GUI mode): please run this on VM (with GUI desktop).

```bash
mpirun -np $n_processes ./mpig $n_body $n_iterations
```


Pthread (command line mode):

```bash
./pthread $n_body $n_iterations $n_threads
```

Pthread (GUI mode): please run this on VM (with GUI desktop).

```bash
./pthreadg $n_body $n_iterations $n_threads
```

CUDA (command line mode): for VM users, please run this on cluster.

```bash
./cuda $n_body $n_iterations
```

CUDA (GUI mode): if you have both nvcc and GUI desktop, you can try this.

```bash
./cuda $n_body $n_iterations
```


OpenMP (command line mode):

```bash
openmp $n_body $n_iterations $n_omp_threads
```

OpenMP (GUI mode):

```bash
openmpg $n_body $n_iterations $n_omp_threads
```

<br/>
<br/>
<br/>

# Makefile

Makefile helps you simplify compilation command.

```bash
make $command
```

where `command` is one of `seq, seqg, mpi, mpig, pthread, pthreadg, cuda, cudag, openmp, openmpg`.


When you need to recompile, please first run `make clean`!



<br/>
<br/>
<br/>

# Advertisement: Valgrind

Valgrind is a memory mismanagement detector. It shows you memory leaks, deallocation errors, etc. Actually, Valgrind is a wrapper around a collection of tools that do many other things (e.g., cache profiling); however, here we focus on the default tool, memcheck. Memcheck can detect:

- Use of uninitialised memory

- Reading/writing memory after it has been free'd

- Reading/writing off the end of malloc'd blocks

- Reading/writing inappropriate areas on the stack

- Memory leaks -- where pointers to malloc'd blocks are lost forever

- Mismatched use of malloc/new/new [] vs free/delete/delete []

- Overlapping src and dst pointers in memcpy() and related functions

- Some misuses of the POSIX pthreads API

To use this on our example program, test.c, try

```bash
gcc -o test -g test.c
```

This creates an executable named test. To check for memory leaks during the execution of test, try

```bash
valgrind --tool=memcheck --leak-check=yes --show-reachable=yes --num-callers=20 --track-fds=yes ./test
```


This outputs a report to the terminal like

```
=9704== Memcheck, a memory error detector for x86-linux.
==9704== Copyright (C) 2002-2004, and GNU GPL'd, by Julian Seward et al.
==9704== Using valgrind-2.2.0, a program supervision framework for x86-linux.
==9704== Copyright (C) 2000-2004, and GNU GPL'd, by Julian Seward et al.
==9704== For more details, rerun with: -v
==9704== 
==9704== 
==9704== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 11 from 1)
==9704== malloc/free: in use at exit: 35 bytes in 2 blocks.
==9704== malloc/free: 3 allocs, 1 frees, 47 bytes allocated.
==9704== For counts of detected errors, rerun with: -v
==9704== searching for pointers to 2 not-freed blocks.
==9704== checked 1420940 bytes.
==9704== 
==9704== 16 bytes in 1 blocks are definitely lost in loss record 1 of 2
==9704==    at 0x1B903D38: malloc (vg_replace_malloc.c:131)
==9704==    by 0x80483BF: main (test.c:15)
==9704== 
==9704== 
==9704== 19 bytes in 1 blocks are definitely lost in loss record 2 of 2
==9704==    at 0x1B903D38: malloc (vg_replace_malloc.c:131)
==9704==    by 0x8048391: main (test.c:8)
==9704== 
==9704== LEAK SUMMARY:
==9704==    definitely lost: 35 bytes in 2 blocks.
==9704==    possibly lost:   0 bytes in 0 blocks.
==9704==    still reachable: 0 bytes in 0 blocks.
==9704==         suppressed: 0 bytes in 0 blocks.

```

Reference: 

[1] http://cs.ecs.baylor.edu/~donahoo/tools/valgrind/

[2] http://senlinzhan.github.io/2017/12/31/valgrind/



<br/>
<br/>
<br/>



# Sbatch script

For example, we want to use 20 cores for experiment.

## MPI

For MPI program, you can use

```sh
#!/bin/bash
#SBATCH --job-name=your_job_name # Job name
#SBATCH --nodes=1                    # Run all processes on a single node	
#SBATCH --ntasks=20                   # number of processes = 20
#SBATCH --cpus-per-task=1      # Number of CPU cores allocated to each process (please use 1 here, in comparison with pthread)
#SBATCH --partition=Project            # Partition name: Project or Debug (Debug is default)

cd /nfsmnt/119010355/CSC4005_2022Fall_Demo/project3_template/
mpirun -np 4 ./mpi 1000 100
mpirun -np 20 ./mpi 1000 100
mpirun -np 40 ./mpi 1000 100


```

## Pthread

For pthread program, you can use

```sh
#!/bin/bash
#SBATCH --job-name=your_job_name # Job name
#SBATCH --nodes=1                    # Run all processes on a single node	
#SBATCH --ntasks=1                   # number of processes = 1 
#SBATCH --cpus-per-task=20      # Number of CPU cores allocated to each process
#SBATCH --partition=Project            # Partition name: Project or Debug (Debug is default)

cd /nfsmnt/119010355/CSC4005_2022Fall_Demo/project3_template/
./pthread 1000 100 4
./pthread 1000 100 20
./pthread 1000 100 40
./pthread 1000 100 80
./pthread 1000 100 120
./pthread 1000 100 200
...

```

here you can create as many threads as you want while the number of cpu cores are fixed.

For a pthread program, we notice that sbatch script contains

```sh
#SBATCH --ntasks=1                   # number of processes = 1 
#SBATCH --cpus-per-task=20      # Number of CPU cores allocated to each process
```

the meaning of these two lines are: only one process is started, it can create many threads, where threads are distributed to all available 20 cpu cores by OS. 


## CUDA


For CUDA program, you can use

```bash
#!/bin/bash

#SBATCH --job-name CSC3150CUDADemo  ## Job name
#SBATCH --gres=gpu:1                ## Number of GPUs required for job execution.
#SBATCH --output result.out         ## filename of the output
#SBATCH --partition=Project           ## the partitions to run in (Debug or Project)
#SBATCH --ntasks=1                  ## number of tasks (analyses) to run
#SBATCH --gpus-per-task=1           ## number of gpus per task
#SBATCH --time=0-00:02:00           ## time for analysis (day-hour:min:sec)

## Compile the cuda script using the nvcc compiler
## You can compile your codes out of the script and simply srun the executable file.
cd /nfsmnt/119010355/CSC4005_2022Fall_Demo/project3_template/
## Run the script
srun ./cuda 10000 100
```


## OpenMP

For OpenMP program, you can use

```bash
#!/bin/bash

#SBATCH --job-name job_name  ## Job name
#SBATCH --output result.out         ## filename of the output
#SBATCH --partition=Project           ## the partitions to run in (Debug or Project)
#SBATCH --ntasks=1                  ## number of tasks (analyses) to run
#SBATCH --gpus-per-task=1           ## number of gpus per task
#SBATCH --time=0-00:02:00           ## time for analysis (day-hour:min:sec)

## Compile the cuda script using the nvcc compiler
## You can compile your codes out of the script and simply srun the executable file.
cd /nfsmnt/119010355/CSC4005_2022Fall_Demo/project3_template/
## Run the script
./openmp 10000 100 20
```


To submit your job, use

```sh
sbatch xxx.sh
```

<br/>
<br/>

# Salloc

If you want to run your program using interactive mode, use

## MPI

For MPI porgram, we have learned before:

```sh
salloc -n20 -c1 # -c1 can be omitted.
mpirun -np 20 ./mpi 1000 1000 100
```

## Pthread
For pthread program,

```sh
salloc -n1 -c20 -p Project # we have only 1 process, 20 is the number of cores allocated per process. 
srun ./pthread 1000 1000 100 20 # 20 is the number of threads.
```

## CUDA

For CUDA program,

```bash
salloc -n1 -c1 --gres=gpu:1 -p Project # require 1 gpu
srun ./cuda 10000 1000
```


## OpenMP

For openMP program,

```bash
salloc -n1 -c20 -p Project # require 1 gpu
srun ./openmp 10000 1000 20
```



<br/>
<br/>
<br/>

# Authors

Bokai Xu

Thank @Peilin Li and @Yangyang Peng for giving valuable suggestions to this series of templates.